{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HybridTestFramework","text":""},{"location":"#architecture","title":"Architecture","text":"<p>In the era of cloud-native world we cannot stick to a particular framework, however due to projects requirement we often need to evolve the existing testing solution in such a way so that it can cater multiple testing requirement, hence HybridTestFramework is targeting to create a bridge between the kind of legacy systems or the systems which are still in a transition phase of migrate to cloud with super cool cloud-native systems. Also, it's worth to mention as we are trying to follow the pattern of testing pyramid where the testing is more focused for the api followed by WebUI, in future the framework focus will be more towards the apis and events.</p>"},{"location":"#framework-capabilities","title":"Framework Capabilities","text":"<ul> <li>Cross browser testing support.</li> <li>Added browserstack support for CrossBrowser testing.</li> <li>Running tests in docker containers selenium grid.</li> <li>Running tests in AWS DeviceFarm selenium grid.</li> <li>Running tests in selenium server in docker containers.</li> <li>Security testing using OWASP, running in docker container.</li> <li>Rest Api and GraphQL testing support powered by RestAssured.</li> <li>gRPC api testing support using native gRPC=java library.</li> <li>Event driven microservice testing based on pubsub model.</li> <li>Support for Kafka, Cloud Pubsub, AWS SNS testing and continue evolving.</li> <li>Visual regression testing using percy.io.</li> <li>Accessibility testing using axe-selenium.</li> <li>Stubbed api testing using WireMock.</li> <li>Can send logs to ElasticSearch for kibana dashboard visualization.</li> <li>Database testing support.</li> <li>Kubernetes support.</li> </ul>"},{"location":"#github-actions-execution","title":"GitHub actions execution","text":""},{"location":"#jenkinsexecution","title":"JenkinsExecution","text":""},{"location":"#azure-devops-testresults","title":"Azure devops TestResults","text":""},{"location":"#allure-reporting","title":"Allure Reporting","text":""},{"location":"#browserstack-dashboard","title":"BrowserStack Dashboard","text":""},{"location":"api/","title":"Api Testing","text":""},{"location":"api/#write-your-first-user-journey-api-test","title":"Write your first user journey api test","text":"<p>Create new class and name as the TC00*_E2E_TEST-***</p> <ul> <li>Provide jira link in @Link</li> <li>Provide all the api components as @Feature</li> <li>Provide test severity and description</li> <li>Write test</li> <li>Use CatchBlock in try/catch section</li> </ul> <p>Create pojo for the request body and deserialize the response. By using the project lombok it's easy to create the data model.</p> <pre><code>@Data\n@AllArgsConstructor\n@NoArgsConstructor\n@Jacksonized\npublic class Booking {\n    private String firstname;\n    private String lastname;\n    private int totalprice;\n    private boolean depositpaid;\n    private Bookingdates bookingdates;\n    private String additionalneeds;\n}\n</code></pre> <pre><code>@Severity(SeverityLevel.CRITICAL)\n@Test(description = \"E2E test for Trading Coins\")\n@Description(\"Get Trading Coins\")\n@Story(\"Test CryptoCoins\")\npublic void TestTradings(){\n    setBaseURI(\"https://api.coingecko.com\");\n\n    Response response=httpGet(\"/api/v3/search/trending\");\n    Assert.assertEquals(getStatusCode(response) /*actual value*/,200 /*expected value*/,\"Correct status code returned\");\n    Trades trades=response.getBody().as(Trades.class);\n    Assert.assertNotNull(trades.getCoins().get(0).item.name);\n    Assert.assertNotNull(trades.getCoins().get(0).item.slug);\n}\n</code></pre>"},{"location":"aws/","title":"AWS Cloud Testing","text":""},{"location":"aws/#write-a-document-how-to-add-a-test","title":"write a document how to add a test","text":""},{"location":"graphql/","title":"GraphQL api Testing","text":""},{"location":"graphql/#write-your-first-user-journey-graph-api-test","title":"Write your first user journey graph api test","text":"<p>Create new graph api test by using the following test conventions</p> <pre><code>@Severity(SeverityLevel.NORMAL)\n@Test(description = \"E2E test for graphql\")\n@Description(\"Get Fruit Shop\")\n@Story(\"Test Graphql\")\npublic void TestFruitShop(){\n        String query=\"query{\\n\"+\n        \"  products(id: \\\"7\\\") {\\n\"+\n        \"    name\\n\"+\n        \"    price\\n\"+\n        \"    category {\\n\"+\n        \"      name\\n\"+\n        \"    }\\n\"+\n        \"    vendor {\\n\"+\n        \"      name\\n\"+\n        \"      id\\n\"+\n        \"    }\\n\"+\n        \"  }\\n\"+\n        \"}\";\n        String jsonString=graphqlToJson(query);\n\n        setBaseURI(\"https://www.predic8.de/fruit-shop-graphql?\");\n        RestAssured\n        .given()\n        .contentType(\"application/json\")\n        .body(jsonString)\n        .when().post().then()\n        .assertThat()\n        .statusLine(\"HTTP/1.1 200 OK\")\n        .log()\n        .body();\n        }\n</code></pre>"},{"location":"grpc/","title":"gRPC testing","text":""},{"location":"grpc/#write-your-first-grpc-testing","title":"Write your first gRPC testing","text":"<p>First get the proto file from the server and put it in <code>contract\\proto</code> folder then the compilation will auto generate the required code.</p> <pre><code>syntax = \"proto3\";\n\npackage greet;\noption go_package = \"greetpb\";\noption java_multiple_files = true;\noption java_outer_classname = \"GreetProto\";\noption java_package = \"com.greet\";\n\nservice GreetService{\n  // Unary\n  rpc Greet(GreetRequest) returns (GreetResponse) {};\n}\n\nmessage Greeting {\n  string first_name = 1;\n  string last_name = 2;\n}\n\nmessage GreetRequest {\n  Greeting greeting = 1;\n}\n\nmessage GreetResponse {\n  string result = 1;\n}\n</code></pre> <p>Create the client code for the rpc connection, then use the client to write the test</p> <pre><code>   CoffeeGrpc.CoffeeBlockingStub coffeeServiceStub;\n\n    public CoffeeClient() throws IOException {\n        ManagedChannel channel = channel(\"coffee-service-i6avjiaelq-ts.a.run.app\", ChannelType.TLS, AuthType.TLS);\n        coffeeServiceStub = CoffeeGrpc.newBlockingStub(channel);\n    }\n\npublic AddCoffeeResponse addCoffee() throws Exception {\n        try {\n            AddCoffeeRequest addCoffeeRequest = AddCoffeeRequest.newBuilder()\n            .setId(UUID.randomUUID().toString())\n            .setName(\"java\")\n            .setFlavour(\"Java Grpc\")\n            .setAroma(\"Hybrid\")\n            .setCoffeeSize(CoffeeSize.SMALL)\n            .setDescription(\"New java coffee client\")\n            .setLastUpdated(Timestamp.newBuilder().build())\n            .build();\n            return coffeeServiceStub.addCoffee(addCoffeeRequest);\n        } catch (Exception e) {\n            return null;\n        }\n}\n</code></pre> <pre><code>private final CoffeeClient coffeeClient;\n\npublic TC009_GrpcApi() throws IOException {\n    coffeeClient = new CoffeeClient();\n}\n\n@Test\npublic void updateCoffee() throws Exception {\n        UpdateCoffeeResponse bookResponse = coffeeClient.updateCoffee();\n        Assert.assertEquals(bookResponse.getMessage(), \"Coffee Details Updated\");\n}\n</code></pre>"},{"location":"kafka/","title":"Kafka Testing","text":""},{"location":"kafka/#what-is-apache-kafka","title":"What is Apache Kafka?","text":"<p>Apache Kafka is a framework implementation of a software bus using stream-processing. It is an open-source software platform developed by the Apache Software Foundation written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Behind the scenes, Kafka is distributed, scales well, replicates data across brokers (servers), can survive broker downtime, and much more. </p> <p></p> <p></p>"},{"location":"kafka/#topics-partitions-and-offsets","title":"Topics, Partitions and Offsets","text":"<p>Topics: A particular stream of data</p> <ul> <li>Similar to a table of the database</li> <li>You can have as many topics you can</li> <li>A topic is identified by its name</li> </ul> <p>Topics are split in partitions</p> <ul> <li>Each partition is ordered</li> <li>Each message in partition will get an incremental ID called offset</li> <li>Partition 0, 1, 2 ....</li> <li>Order only guaranteed within a partition, not across partitions</li> <li>Data is kept only for a limited time.</li> <li>Once the data is written to a partition it cannot be changed.</li> </ul> <p>Example Scenario : You can have multiple cabs, and each cabs reports its GPS location to kafka. You can have a topic cabs_gps that contains the position of all cabs. Each cab will send a message to kafka every 20 sec, each message will contain the cabID, and the cab location(lat/long)</p>"},{"location":"kafka/#brokers-topics","title":"Brokers &amp; Topics","text":"<ul> <li>A kafka cluster is composed of multiple brokers(servers)</li> <li>Each broker is identified by its ID(integer)</li> <li>Each broker contains certain topic partitions</li> <li>After connecting to any broker(called a bootstrap broker), you will be connected to the entire cluster</li> <li>A good number to get start is 3 brokers, but some big clusters have more than 100 brokers</li> </ul> <p>Example of topic A with 3 partitions  Example of topic B with 2 partitions  </p>"},{"location":"kafka/#topics-replication","title":"Topics replication","text":"<ul> <li>Topics should have a replication factor &gt;1 (Usually between 2 and 3)</li> <li> <p>This way if one broker is down another broker can serve the data. Example of topic A with replication factor 2   </p> </li> <li> <p>At any time only ONE broker can be a leader for a given partition</p> </li> <li>Only that leader can receive and serve data for a partition.</li> <li>The other broker will synchronize the data.</li> <li>So each partition has one leader and multiple ISR (in-sync-replica)   </li> </ul>"},{"location":"kafka/#producer","title":"Producer","text":"<ul> <li>Producer write data to topics(which is made of partitions)</li> <li>Producer automatically know to which broker and partition to write.</li> <li>In case broker failure, Producers will automatically recover   </li> <li>Producers can choose to receive acknowledgment of data writes.<ul> <li>acks=0 Producer won't wait for acknowledgment (Possible data loss)</li> <li>acks=1 Producer will wait for leader acknowledgment (Limited data loss)</li> <li>acks=2 Leader &amp; Replica acknowledgment (no data loss)</li> </ul> </li> <li>Producer can choose to send a key with the message(string,num etc.)</li> <li>If key==null data will sent round robin(broker 101 then 102 then 103)</li> <li>If key is sent then all message for that key will send to same partition</li> <li>A key is sent if we need a message ordering for a specific field as cabID.</li> </ul>"},{"location":"kafka/#consumer","title":"Consumer","text":"<ul> <li>Consumer read data from a topic(identified by name)</li> <li>Consumer knows which broker to read from</li> <li>In case of broker failure, consumer know how to recover</li> <li>Data is read in order with in each partition   </li> <li>Consumer read data in consumer groups</li> <li>Each consumer within a group reads form exclusive partitions</li> <li>If you have more consumers than partitions, some consumers will be inactive</li> <li>Kafka stores the offset at which a consumer group has been reading</li> <li>The offsets committed live in a kafka topic named _consumer_offsets</li> <li>When a consumer in a group has processed the data received from kafka, it should be committing the offsets.</li> <li>If a consumer dies, it will be able to read back from where it left off.</li> </ul>"},{"location":"kafka/#zookeeper","title":"Zookeeper","text":"<ul> <li>Zookeeper manager brokers(keeps a list of them)</li> <li>Zookeeper helps in performing leader election for partition</li> <li>Zookeeper send notifications to kafka in case of any changes.</li> </ul>"},{"location":"kafka/#schema-registry","title":"Schema Registry","text":"<ul> <li>Kafka takes bytes as an input and publishes them</li> <li>No data verification</li> <li>Schema registry rejects bat data</li> <li>A common data format must be agreed upon  </li> <li>Apache avro as data format<ul> <li>Data is fully typed</li> <li>Date is compressed automatically</li> <li>Schema comes along with the data</li> <li>Documentation is embedded in the schema</li> <li>Data can be read across any language</li> <li>Schema can be evolved over time in safe manner</li> </ul> </li> </ul>"},{"location":"kafka/#avro","title":"Avro","text":"<p>Apache Avro is a data serialization system.</p> <ul> <li>Avro provides:<ul> <li>Rich data structures.</li> <li>A compact, fast, binary data format.</li> <li>A container file, to store persistent data.</li> <li>Remote procedure call (RPC).</li> <li>Simple integration with dynamic languages. Code generation is not required to read or write data files nor to use   or implement RPC protocols. Code generation as an optional optimization, only worth implementing for statically   typed languages.</li> </ul> </li> </ul> <pre><code>{\"namespace\": \"dip.avro\",\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"favorite_number\",  \"type\": [\"int\", \"null\"]},\n    {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n  ]\n}\n</code></pre> <ul> <li>Common Fields:<ul> <li>Name: Name of the schema</li> <li>Namespace: (equivalent of package in java)</li> <li>Doc: Documentation to explain your schema</li> <li>Aliases: Optional other name for schema</li> <li>Fields<ul> <li>Name: Name of field</li> <li>Doc: Documentation for that field</li> <li>Type: Data type for that field</li> <li>Default: Default value for that field</li> </ul> </li> <li>Complex types:<ul> <li>Enums   <pre><code>{\n  \"type\": \"enum\",\n  \"name\": \"Customer Status\",\n  \"symbols\": [\"BRONZE\",\"SILVER\",\"GOLD\"]\n}\n</code></pre></li> <li>Arrays   <pre><code>{\n  \"type\": \"array\",\n  \"items\": \"string\"\n}\n</code></pre></li> <li>Maps   <pre><code>{\n  \"type\": \"map\",\n  \"values\": \"string\"\n}\n</code></pre></li> <li>Unions   <pre><code>{\n  \"name\": \"middle_name\",\n  \"type\": [\n    \"null\",\n    \"string\"\n  ],\n  \"default\": \"null\"\n}\n</code></pre></li> <li>Calling other schema as type</li> </ul> </li> </ul> </li> </ul>"},{"location":"kafka/#kafka-rest-proxy","title":"Kafka Rest Proxy","text":"<ul> <li>kafka is great for java based consumers/producers</li> <li>Avro support for some languages isn't great, where JSON/HTTP requests are great.</li> <li>Reporting data to Kafka from any frontend app built in any language not supported by official Confluent clients</li> <li>Ingesting messages into a stream processing framework that doesn\u2019t yet support Kafka  </li> <li>Perform a comprehensive set of administrative operations through REST APIs, including:<ul> <li>Describe, list, and configure brokers</li> <li>Create, delete, describe, list, and configure topics</li> <li>Delete, describe, and list consumer groups</li> <li>Create, delete, describe, and list ACLs</li> <li>List partition reassignments     </li> </ul> </li> </ul>"},{"location":"selenium/","title":"Selenium Testing","text":""},{"location":"selenium/#write-your-first-user-journey-web-ui-test","title":"Write your first user journey web ui test","text":"<p>Create new class and name as the TC00*_E2E_TEST-***</p> <ul> <li>Provide jira link in @Link</li> <li>Provide all the api components as @Feature</li> <li>Provide test severity and description</li> <li>Write test</li> <li>Use CatchBlock in try/catch section</li> </ul>"},{"location":"setup/","title":"Project Setup","text":"<ul> <li>Install IntelliJ IDEA   https://www.jetbrains.com/idea/download/</li> <li>Install docker desktop   https://www.docker.com/products/docker-desktop</li> <li>Java JDK_17 https://adoptium.net/temurin/releases/?version=17</li> <li>Gradle   https://gradle.org/next-steps/?version=8.5&amp;format=bin</li> <li>Allure   https://github.com/allure-framework/allure2/archive/2.25.0.zip</li> <li>Set Environment variables<ul> <li>JAVA_HOME: Pointing to the Java SDK folder\\bin</li> <li>GRADLE_HOME: Pointing to Gradle directory\\bin.</li> <li>ALLURE_HOME: Pointing to allure directory\\bin.</li> </ul> </li> </ul>"},{"location":"setup/#java-17-jdk-installation-and-config","title":"Java 17 JDK Installation and config","text":"<ul> <li>Download Java 17 JDK from here</li> <li>Install downloaded Java 17 JDK.</li> <li>System Properties -&gt; Environment Variables -&gt; System Variables -&gt; New -&gt; <code>JAVA_HOME</code> and Value   as <code>C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.9</code> (path where JDK is installed)</li> <li>System Properties -&gt; Environment Variables -&gt; System Variables -&gt; Select <code>Path</code> and Edit -&gt; Add <code>%JAVA_HOME%\\bin</code></li> <li>Once Environment variables are configured, open command prompt and run <code>java -v</code> and <code>echo $JAVA_HOME</code> to check   whether java version installed is returned. e.g  <code>$ java -version</code> <code>$ openjdk 17.0.9 2023-10-17&lt;br/&gt;   OpenJDK Runtime Environment Temurin-17.0.9+9 (build 17.0.9+9)&lt;br/&gt;   OpenJDK 64-Bit Server VM Temurin-17.0.9+9 (build 17.0.9+9, mixed mode, sharing)</code></li> </ul>"},{"location":"setup/#recommended-ide","title":"Recommended IDE","text":"<ul> <li>Please install Community Edition of <code>IntelliJ IDEA</code>   from here</li> </ul>"},{"location":"setup/#getting-started","title":"Getting Started","text":"<p>```shell script $ git clone  $ cd  $ import project from intellij as a gradle project $ gradle clean $ gradle build $ gradle task web $ gradle task mobile $ gradle allureReport $ gradle allureServe <pre><code>### Spawns chrome, firefox, selenium hub and OWASP proxy server\n\n```shell script\n$ docker-compose up -d\n</code></pre></p>"},{"location":"setup/#complete-infrastructure-creation-for-local-run","title":"Complete infrastructure creation for local run","text":"<p>```shell script $ $ docker-compose -f docker-compose-infra up -d <pre><code>### Spawns four additional node-chrome/firefox instances linked to the hub\n\n```shell script\n$ docker-compose scale chrome=5\n$ docker-compose scale firefox=5\n</code></pre></p> <p>Error Handle for dynamic classpath error in intellij: Search and modify the below line in .idea workspace.xml</p> <pre><code>&lt;component name=\"PropertiesComponent\"&gt;\n    &lt;property name=\"dynamic.classpath\" value=\"true\"/&gt;\n&lt;/component&gt;\n</code></pre>"},{"location":"setup/#security-zap-testing","title":"Security ZAP Testing","text":"<p>OWASP ZAP Download it from Github</p> <ul> <li>Run it</li> <li>Configure proxy: Tools -&gt; Options -&gt; Local Proxies. Set port to 8888</li> <li>Get API key from your ZAP instance: Tools -&gt; Options -&gt; API</li> </ul> <p>Vulnerable application - system under test</p> <ul> <li>Install docker and run docker service</li> <li>Run bodgeit docker container (or any app)</li> <li>Make sure it's running on http://localhost:8080/bodgeit/</li> </ul> <p>Selenium traffic will go through ZAP proxy in order to capture all traffic. It's not exactly necessary for the bodgeit shop, but in real-world applications spider would struggle to find URLs requiring logged in access.</p>"}]}